{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6200ad62-d622-4ae1-9259-921afcfe5d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cbf1cb6-7da7-439f-bd43-a699c99043b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import numpy as np\n",
    "import skfda\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from fundl.config import FunDLnBlockConfig, FunDLObjective\n",
    "from fundl.model import FunDL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9781e068-fa08-4b23-9a94-9c69bc888303",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b0f4dd-fdf8-465d-882d-2ccfd5f7ccd0",
   "metadata": {},
   "source": [
    "### Process Data for FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c236bf8-df14-41fa-9c0d-5e04e3ca39d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fda_data, obs_data = skfda.datasets.fetch_tecator(return_X_y=True, as_frame=False)\n",
    "data = np.concatenate(\n",
    "    (\n",
    "        fda_data.data_matrix,\n",
    "        fda_data.derivative().data_matrix,\n",
    "        fda_data.derivative().derivative().data_matrix,\n",
    "    ),\n",
    "    axis=2,\n",
    ")\n",
    "\n",
    "X_train, X_val, obs_data_train, obs_data_val = train_test_split(\n",
    "    data, obs_data, test_size=0.2, random_state=42\n",
    ")\n",
    "y_train = obs_data_train[:,0]\n",
    "y_val = obs_data_val[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525c4e80-39fe-468b-afbb-1637f158f07a",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed57a259-fbb0-461d-ad1d-63e32def0889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"FunDL\"\n",
      "_______________________________________________________________________________________________________________\n",
      " Layer (type)                                Output Shape                            Param #        Trainable  \n",
      "===============================================================================================================\n",
      " input (InputLayer)                          [(None, 100, 3)]                        0              Y          \n",
      "                                                                                                               \n",
      " 01_relu_dropout_16 (DenseDropoutBlock)      (None, 100, 16)                         64             Y          \n",
      "                                                                                                               \n",
      " 02_relu_dropout_16 (DenseDropoutBlock)      (None, 100, 32)                         544            Y          \n",
      "                                                                                                               \n",
      " 03_relu_dropout_16 (DenseDropoutBlock)      (None, 100, 16)                         528            Y          \n",
      "                                                                                                               \n",
      " 04_relu_16 (DenseDropoutBlock)              (None, 100, 8)                          136            Y          \n",
      "                                                                                                               \n",
      " output (Dense)                              (None, 100, 1)                          9              Y          \n",
      "                                                                                                               \n",
      "===============================================================================================================\n",
      "Total params: 1,281\n",
      "Trainable params: 1,281\n",
      "Non-trainable params: 0\n",
      "_______________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "hidden_layers = [\n",
    "    FunDLnBlockConfig(\n",
    "        name=\"01_relu_dropout_16\",\n",
    "        units=16,\n",
    "        activation=\"relu\",\n",
    "        dropout_rate=None,\n",
    "    ),\n",
    "    FunDLnBlockConfig(\n",
    "        name=\"02_relu_dropout_16\",\n",
    "        units=32,\n",
    "        activation=\"relu\",\n",
    "        dropout_rate=None,\n",
    "    ),\n",
    "    FunDLnBlockConfig(\n",
    "        name=\"03_relu_dropout_16\",\n",
    "        units=16,\n",
    "        activation=\"relu\",\n",
    "        dropout_rate=None,\n",
    "    ),\n",
    "    FunDLnBlockConfig(\n",
    "        name=\"04_relu_16\",\n",
    "        units=8,\n",
    "        activation=\"relu\",\n",
    "        add_dropout=False,\n",
    "        dropout_rate=None,\n",
    "    ),\n",
    "]\n",
    "input_shape = 100\n",
    "output_size = 1\n",
    "model = FunDL(\n",
    "    objective=FunDLObjective(\"classification\"),\n",
    "    input_shape=input_shape,\n",
    "    hidden_layers=hidden_layers,\n",
    "    output_size=output_size,\n",
    ")\n",
    "model.compile(\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001, decay = 0),\n",
    "    loss=\"mse\", \n",
    "    metrics=[\"mean_squared_error\"]\n",
    "\n",
    ")\n",
    "_ = model.summary(line_length=100, show_trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcfba38d-ff2a-4e70-984b-f279b959204c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 449.1162 - mean_squared_error: 449.1162\n",
      "Epoch 2/50\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 449.1164 - mean_squared_error: 449.1165\n",
      "Epoch 3/50\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 449.1164 - mean_squared_error: 449.1163\n",
      "Epoch 4/50\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 449.1163 - mean_squared_error: 449.1163\n",
      "Epoch 5/50\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 449.1163 - mean_squared_error: 449.1162\n",
      "Epoch 6/50\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 449.1164 - mean_squared_error: 449.1164\n",
      "Epoch 7/50\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 449.1161 - mean_squared_error: 449.1162\n",
      "Epoch 8/50\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 449.1165 - mean_squared_error: 449.1165\n",
      "Epoch 9/50\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 449.1161 - mean_squared_error: 449.1162\n",
      "Epoch 10/50\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 449.1165 - mean_squared_error: 449.1164\n",
      "Epoch 11/50\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 449.1163 - mean_squared_error: 449.1163\n",
      "Epoch 12/50\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 449.1164 - mean_squared_error: 449.1165\n",
      "Epoch 13/50\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 449.1164 - mean_squared_error: 449.1164\n",
      "Epoch 14/50\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 449.1163 - mean_squared_error: 449.1163\n",
      "Epoch 15/50\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 449.1164 - mean_squared_error: 449.1164\n",
      "Epoch 16/50\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 449.1161 - mean_squared_error: 449.1161\n",
      "Epoch 17/50\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 449.1164 - mean_squared_error: 449.1164\n",
      "Epoch 18/50\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 449.1164 - mean_squared_error: 449.1164\n",
      "Epoch 19/50\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 449.1166 - mean_squared_error: 449.1166\n",
      "Epoch 20/50\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 449.1162 - mean_squared_error: 449.1162\n",
      "Epoch 21/50\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 449.1164 - mean_squared_error: 449.1164\n",
      "Epoch 22/50\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 449.1163 - mean_squared_error: 449.1163\n",
      "Epoch 23/50\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 449.1162 - mean_squared_error: 449.1162\n",
      "Epoch 24/50\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 449.1165 - mean_squared_error: 449.1164\n",
      "Epoch 25/50\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 449.1165 - mean_squared_error: 449.1165\n",
      "Epoch 26/50\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 449.1164 - mean_squared_error: 449.1165\n",
      "Epoch 27/50\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 449.1161 - mean_squared_error: 449.1161\n",
      "Epoch 28/50\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 449.1161 - mean_squared_error: 449.1161\n",
      "Epoch 29/50\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 449.1165 - mean_squared_error: 449.1164\n",
      "Epoch 30/50\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 449.1165 - mean_squared_error: 449.1165\n",
      "Epoch 31/50\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 449.1162 - mean_squared_error: 449.1162\n",
      "Epoch 32/50\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 449.1163 - mean_squared_error: 449.1164\n",
      "Epoch 33/50\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 449.1163 - mean_squared_error: 449.1163\n",
      "Epoch 34/50\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 449.1162 - mean_squared_error: 449.1162\n",
      "Epoch 35/50\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 449.1161 - mean_squared_error: 449.1161\n",
      "Epoch 36/50\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 449.1162 - mean_squared_error: 449.1163\n",
      "Epoch 37/50\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 449.1163 - mean_squared_error: 449.1164\n",
      "Epoch 38/50\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 449.1164 - mean_squared_error: 449.1164\n",
      "Epoch 39/50\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 449.1163 - mean_squared_error: 449.1163\n",
      "Epoch 40/50\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 449.1162 - mean_squared_error: 449.1162\n",
      "Epoch 41/50\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 449.1163 - mean_squared_error: 449.1163\n",
      "Epoch 42/50\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 449.1163 - mean_squared_error: 449.1162\n",
      "Epoch 43/50\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 449.1162 - mean_squared_error: 449.1161\n",
      "Epoch 44/50\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 449.1162 - mean_squared_error: 449.1162\n",
      "Epoch 45/50\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 449.1166 - mean_squared_error: 449.1166\n",
      "Epoch 46/50\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 449.1164 - mean_squared_error: 449.1163\n",
      "Epoch 47/50\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 449.1165 - mean_squared_error: 449.1166\n",
      "Epoch 48/50\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 449.1164 - mean_squared_error: 449.1164\n",
      "Epoch 49/50\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 449.1162 - mean_squared_error: 449.1162\n",
      "Epoch 50/50\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 449.1165 - mean_squared_error: 449.1165\n"
     ]
    }
   ],
   "source": [
    "_ = model.fit(X_train, y_train, batch_size=16, epochs=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93c9c686-12a3-42cd-a251-915fb98c0321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trains the model for a fixed number of epochs (iterations on a dataset).\n",
      "\n",
      "    Args:\n",
      "        x: Input data. It could be:\n",
      "          - A Numpy array (or array-like), or a list of arrays\n",
      "            (in case the model has multiple inputs).\n",
      "          - A TensorFlow tensor, or a list of tensors\n",
      "            (in case the model has multiple inputs).\n",
      "          - A dict mapping input names to the corresponding array/tensors,\n",
      "            if the model has named inputs.\n",
      "          - A `tf.data` dataset. Should return a tuple\n",
      "            of either `(inputs, targets)` or\n",
      "            `(inputs, targets, sample_weights)`.\n",
      "          - A generator or `keras.utils.Sequence` returning `(inputs, targets)`\n",
      "            or `(inputs, targets, sample_weights)`.\n",
      "          - A `tf.keras.utils.experimental.DatasetCreator`, which wraps a\n",
      "            callable that takes a single argument of type\n",
      "            `tf.distribute.InputContext`, and returns a `tf.data.Dataset`.\n",
      "            `DatasetCreator` should be used when users prefer to specify the\n",
      "            per-replica batching and sharding logic for the `Dataset`.\n",
      "            See `tf.keras.utils.experimental.DatasetCreator` doc for more\n",
      "            information.\n",
      "          A more detailed description of unpacking behavior for iterator types\n",
      "          (Dataset, generator, Sequence) is given below. If using\n",
      "          `tf.distribute.experimental.ParameterServerStrategy`, only\n",
      "          `DatasetCreator` type is supported for `x`.\n",
      "        y: Target data. Like the input data `x`,\n",
      "          it could be either Numpy array(s) or TensorFlow tensor(s).\n",
      "          It should be consistent with `x` (you cannot have Numpy inputs and\n",
      "          tensor targets, or inversely). If `x` is a dataset, generator,\n",
      "          or `keras.utils.Sequence` instance, `y` should\n",
      "          not be specified (since targets will be obtained from `x`).\n",
      "        batch_size: Integer or `None`.\n",
      "            Number of samples per gradient update.\n",
      "            If unspecified, `batch_size` will default to 32.\n",
      "            Do not specify the `batch_size` if your data is in the\n",
      "            form of datasets, generators, or `keras.utils.Sequence` instances\n",
      "            (since they generate batches).\n",
      "        epochs: Integer. Number of epochs to train the model.\n",
      "            An epoch is an iteration over the entire `x` and `y`\n",
      "            data provided\n",
      "            (unless the `steps_per_epoch` flag is set to\n",
      "            something other than None).\n",
      "            Note that in conjunction with `initial_epoch`,\n",
      "            `epochs` is to be understood as \"final epoch\".\n",
      "            The model is not trained for a number of iterations\n",
      "            given by `epochs`, but merely until the epoch\n",
      "            of index `epochs` is reached.\n",
      "        verbose: 'auto', 0, 1, or 2. Verbosity mode.\n",
      "            0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
      "            'auto' defaults to 1 for most cases, but 2 when used with\n",
      "            `ParameterServerStrategy`. Note that the progress bar is not\n",
      "            particularly useful when logged to a file, so verbose=2 is\n",
      "            recommended when not running interactively (eg, in a production\n",
      "            environment).\n",
      "        callbacks: List of `keras.callbacks.Callback` instances.\n",
      "            List of callbacks to apply during training.\n",
      "            See `tf.keras.callbacks`. Note `tf.keras.callbacks.ProgbarLogger`\n",
      "            and `tf.keras.callbacks.History` callbacks are created automatically\n",
      "            and need not be passed into `model.fit`.\n",
      "            `tf.keras.callbacks.ProgbarLogger` is created or not based on\n",
      "            `verbose` argument to `model.fit`.\n",
      "            Callbacks with batch-level calls are currently unsupported with\n",
      "            `tf.distribute.experimental.ParameterServerStrategy`, and users are\n",
      "            advised to implement epoch-level calls instead with an appropriate\n",
      "            `steps_per_epoch` value.\n",
      "        validation_split: Float between 0 and 1.\n",
      "            Fraction of the training data to be used as validation data.\n",
      "            The model will set apart this fraction of the training data,\n",
      "            will not train on it, and will evaluate\n",
      "            the loss and any model metrics\n",
      "            on this data at the end of each epoch.\n",
      "            The validation data is selected from the last samples\n",
      "            in the `x` and `y` data provided, before shuffling. This argument is\n",
      "            not supported when `x` is a dataset, generator or\n",
      "           `keras.utils.Sequence` instance.\n",
      "            `validation_split` is not yet supported with\n",
      "            `tf.distribute.experimental.ParameterServerStrategy`.\n",
      "        validation_data: Data on which to evaluate\n",
      "            the loss and any model metrics at the end of each epoch.\n",
      "            The model will not be trained on this data. Thus, note the fact\n",
      "            that the validation loss of data provided using `validation_split`\n",
      "            or `validation_data` is not affected by regularization layers like\n",
      "            noise and dropout.\n",
      "            `validation_data` will override `validation_split`.\n",
      "            `validation_data` could be:\n",
      "              - A tuple `(x_val, y_val)` of Numpy arrays or tensors.\n",
      "              - A tuple `(x_val, y_val, val_sample_weights)` of NumPy arrays.\n",
      "              - A `tf.data.Dataset`.\n",
      "              - A Python generator or `keras.utils.Sequence` returning\n",
      "              `(inputs, targets)` or `(inputs, targets, sample_weights)`.\n",
      "            `validation_data` is not yet supported with\n",
      "            `tf.distribute.experimental.ParameterServerStrategy`.\n",
      "        shuffle: Boolean (whether to shuffle the training data\n",
      "            before each epoch) or str (for 'batch'). This argument is ignored\n",
      "            when `x` is a generator or an object of tf.data.Dataset.\n",
      "            'batch' is a special option for dealing\n",
      "            with the limitations of HDF5 data; it shuffles in batch-sized\n",
      "            chunks. Has no effect when `steps_per_epoch` is not `None`.\n",
      "        class_weight: Optional dictionary mapping class indices (integers)\n",
      "            to a weight (float) value, used for weighting the loss function\n",
      "            (during training only).\n",
      "            This can be useful to tell the model to\n",
      "            \"pay more attention\" to samples from\n",
      "            an under-represented class.\n",
      "        sample_weight: Optional Numpy array of weights for\n",
      "            the training samples, used for weighting the loss function\n",
      "            (during training only). You can either pass a flat (1D)\n",
      "            Numpy array with the same length as the input samples\n",
      "            (1:1 mapping between weights and samples),\n",
      "            or in the case of temporal data,\n",
      "            you can pass a 2D array with shape\n",
      "            `(samples, sequence_length)`,\n",
      "            to apply a different weight to every timestep of every sample. This\n",
      "            argument is not supported when `x` is a dataset, generator, or\n",
      "           `keras.utils.Sequence` instance, instead provide the sample_weights\n",
      "            as the third element of `x`.\n",
      "        initial_epoch: Integer.\n",
      "            Epoch at which to start training\n",
      "            (useful for resuming a previous training run).\n",
      "        steps_per_epoch: Integer or `None`.\n",
      "            Total number of steps (batches of samples)\n",
      "            before declaring one epoch finished and starting the\n",
      "            next epoch. When training with input tensors such as\n",
      "            TensorFlow data tensors, the default `None` is equal to\n",
      "            the number of samples in your dataset divided by\n",
      "            the batch size, or 1 if that cannot be determined. If x is a\n",
      "            `tf.data` dataset, and 'steps_per_epoch'\n",
      "            is None, the epoch will run until the input dataset is exhausted.\n",
      "            When passing an infinitely repeating dataset, you must specify the\n",
      "            `steps_per_epoch` argument. If `steps_per_epoch=-1` the training\n",
      "            will run indefinitely with an infinitely repeating dataset.\n",
      "            This argument is not supported with array inputs.\n",
      "            When using `tf.distribute.experimental.ParameterServerStrategy`:\n",
      "              * `steps_per_epoch=None` is not supported.\n",
      "        validation_steps: Only relevant if `validation_data` is provided and\n",
      "            is a `tf.data` dataset. Total number of steps (batches of\n",
      "            samples) to draw before stopping when performing validation\n",
      "            at the end of every epoch. If 'validation_steps' is None, validation\n",
      "            will run until the `validation_data` dataset is exhausted. In the\n",
      "            case of an infinitely repeated dataset, it will run into an\n",
      "            infinite loop. If 'validation_steps' is specified and only part of\n",
      "            the dataset will be consumed, the evaluation will start from the\n",
      "            beginning of the dataset at each epoch. This ensures that the same\n",
      "            validation samples are used every time.\n",
      "        validation_batch_size: Integer or `None`.\n",
      "            Number of samples per validation batch.\n",
      "            If unspecified, will default to `batch_size`.\n",
      "            Do not specify the `validation_batch_size` if your data is in the\n",
      "            form of datasets, generators, or `keras.utils.Sequence` instances\n",
      "            (since they generate batches).\n",
      "        validation_freq: Only relevant if validation data is provided. Integer\n",
      "            or `collections.abc.Container` instance (e.g. list, tuple, etc.).\n",
      "            If an integer, specifies how many training epochs to run before a\n",
      "            new validation run is performed, e.g. `validation_freq=2` runs\n",
      "            validation every 2 epochs. If a Container, specifies the epochs on\n",
      "            which to run validation, e.g. `validation_freq=[1, 2, 10]` runs\n",
      "            validation at the end of the 1st, 2nd, and 10th epochs.\n",
      "        max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n",
      "            input only. Maximum size for the generator queue.\n",
      "            If unspecified, `max_queue_size` will default to 10.\n",
      "        workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
      "            only. Maximum number of processes to spin up\n",
      "            when using process-based threading. If unspecified, `workers`\n",
      "            will default to 1.\n",
      "        use_multiprocessing: Boolean. Used for generator or\n",
      "            `keras.utils.Sequence` input only. If `True`, use process-based\n",
      "            threading. If unspecified, `use_multiprocessing` will default to\n",
      "            `False`. Note that because this implementation relies on\n",
      "            multiprocessing, you should not pass non-picklable arguments to\n",
      "            the generator as they can't be passed easily to children processes.\n",
      "\n",
      "    Unpacking behavior for iterator-like inputs:\n",
      "        A common pattern is to pass a tf.data.Dataset, generator, or\n",
      "      tf.keras.utils.Sequence to the `x` argument of fit, which will in fact\n",
      "      yield not only features (x) but optionally targets (y) and sample weights.\n",
      "      Keras requires that the output of such iterator-likes be unambiguous. The\n",
      "      iterator should return a tuple of length 1, 2, or 3, where the optional\n",
      "      second and third elements will be used for y and sample_weight\n",
      "      respectively. Any other type provided will be wrapped in a length one\n",
      "      tuple, effectively treating everything as 'x'. When yielding dicts, they\n",
      "      should still adhere to the top-level tuple structure.\n",
      "      e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate\n",
      "      features, targets, and weights from the keys of a single dict.\n",
      "        A notable unsupported data type is the namedtuple. The reason is that\n",
      "      it behaves like both an ordered datatype (tuple) and a mapping\n",
      "      datatype (dict). So given a namedtuple of the form:\n",
      "          `namedtuple(\"example_tuple\", [\"y\", \"x\"])`\n",
      "      it is ambiguous whether to reverse the order of the elements when\n",
      "      interpreting the value. Even worse is a tuple of the form:\n",
      "          `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])`\n",
      "      where it is unclear if the tuple was intended to be unpacked into x, y,\n",
      "      and sample_weight or passed through as a single element to `x`. As a\n",
      "      result the data processing code will simply raise a ValueError if it\n",
      "      encounters a namedtuple. (Along with instructions to remedy the issue.)\n",
      "\n",
      "    Returns:\n",
      "        A `History` object. Its `History.history` attribute is\n",
      "        a record of training loss values and metrics values\n",
      "        at successive epochs, as well as validation loss values\n",
      "        and validation metrics values (if applicable).\n",
      "\n",
      "    Raises:\n",
      "        RuntimeError: 1. If the model was never compiled or,\n",
      "        2. If `model.fit` is  wrapped in `tf.function`.\n",
      "\n",
      "        ValueError: In case of mismatch between the provided input data\n",
      "            and what the model expects or when the input data is empty.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(model.fit.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a81b1bb-677b-4815-a6f2-8071bcf64381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.65392206e-05],\n",
       "        [ 1.77643480e-05],\n",
       "        [ 2.38899853e-05],\n",
       "        ...,\n",
       "        [-2.08271667e-05],\n",
       "        [-9.80101960e-06],\n",
       "        [ 9.80101960e-06]],\n",
       "\n",
       "       [[-2.14397304e-05],\n",
       "        [-3.06281863e-06],\n",
       "        [ 1.59266569e-05],\n",
       "        ...,\n",
       "        [-2.29711397e-04],\n",
       "        [-2.04596284e-04],\n",
       "        [-1.72742970e-04]],\n",
       "\n",
       "       [[-1.40889657e-05],\n",
       "        [-6.12563725e-07],\n",
       "        [ 1.34764020e-05],\n",
       "        ...,\n",
       "        [ 1.28025819e-04],\n",
       "        [ 1.10261471e-04],\n",
       "        [ 9.55599411e-05]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 3.49161323e-05],\n",
       "        [ 3.36910049e-05],\n",
       "        [ 3.36910049e-05],\n",
       "        ...,\n",
       "        [-1.65392206e-04],\n",
       "        [-1.38439402e-04],\n",
       "        [-1.01685578e-04]],\n",
       "\n",
       "       [[ 6.12563725e-05],\n",
       "        [ 4.16543333e-05],\n",
       "        [ 3.24658774e-05],\n",
       "        ...,\n",
       "        [-2.60339583e-04],\n",
       "        [-2.34611907e-04],\n",
       "        [-2.06433975e-04]],\n",
       "\n",
       "       [[ 2.63402402e-05],\n",
       "        [ 4.22668970e-05],\n",
       "        [ 6.18689362e-05],\n",
       "        ...,\n",
       "        [-4.41658446e-04],\n",
       "        [-3.96328730e-04],\n",
       "        [-3.38747740e-04]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_func.derivative().derivative().data_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac42b2c-1c10-4224-9278-de93cd10f17c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
